"""Schema for evaluation prompts and metrics."""

from __future__ import annotations

from enum import Enum
from typing import Any

from pydantic import BaseModel, ConfigDict, Field


class EvalCategory(str, Enum):
    """Top-level evaluation categories."""

    SHORT = "short"
    LONG = "long"
    OOD = "ood"  # out-of-distribution
    TYPO = "typo"
    CONTROLLABILITY = "controllability"


class ControllabilitySubcategory(str, Enum):
    """Subcategories for controllability evaluation."""

    TEMPO = "tempo"
    MODE = "mode"
    RHYTHM = "rhythm"
    TEXTURE = "texture"
    BRIGHTNESS = "brightness"


class EvalDifficulty(str, Enum):
    """Difficulty levels for evaluation prompts."""

    EASY = "easy"
    MEDIUM = "medium"
    HARD = "hard"


class EvalSource(str, Enum):
    """How the evaluation prompt was created."""

    MANUAL = "manual"  # Hand-crafted by human
    LLM_GENERATED = "llm_generated"  # Generated by LLM
    EXTRACTED = "extracted"  # Extracted from training data
    CORRUPTED = "corrupted"  # Programmatically corrupted from clean prompt


class ErrorTag(str, Enum):
    """Error tags for tracking what went wrong during evaluation."""

    # Config generation errors (vibe-to-config model output)
    CONFIG_JSON_INVALID = "config_json_invalid"  # Model output wasn't valid JSON
    CONFIG_SCHEMA_INVALID = "config_schema_invalid"  # JSON didn't match MusicConfigPromptPayload

    # CLAP scoring errors
    CLAP_FAILED = "clap_failed"  # CLAP scoring threw an exception

    # LLM scoring errors
    LLM_SCHEMA_ERROR = "llm_schema_error"  # LLM returned scores outside valid schema
    LLM_FAILED = "llm_failed"  # LLM API error or other failure


class EvalPrompt(BaseModel):
    """Schema for a single evaluation prompt."""

    model_config = ConfigDict(extra="forbid")

    id: str = Field(..., description="Unique identifier for this prompt")
    prompt: str = Field(..., description="The vibe text to evaluate")
    category: EvalCategory = Field(..., description="Top-level category")
    subcategory: ControllabilitySubcategory | None = Field(
        default=None,
        description="Subcategory for controllability prompts",
    )
    expected_fields: dict[str, str] = Field(
        default_factory=dict,
        description="Expected config field values (e.g., {'tempo': 'slow'})",
    )
    difficulty: EvalDifficulty = Field(
        default=EvalDifficulty.MEDIUM,
        description="Subjective difficulty level",
    )
    source: EvalSource = Field(..., description="How this prompt was created")
    paraphrase_group: str | None = Field(
        default=None,
        description="Group ID for paraphrased variants of same intent",
    )
    notes: str | None = Field(default=None, description="Optional notes about this prompt")


class EvalResult(BaseModel):
    """Result from evaluating a single prompt."""

    model_config = ConfigDict(extra="forbid")

    prompt_id: str
    source_label: str  # Which model/baseline produced this
    config: dict[str, Any] | None = Field(default=None)
    config_error: str | None = Field(default=None)

    # Validity metrics
    json_valid: bool = False
    schema_valid: bool = False
    field_matches: dict[str, bool] = Field(default_factory=dict)

    # CLAP metrics (if computed)
    clap_score: float | None = None
    clap_audio_text_sim: float | None = None
    clap_badness_penalty: float | None = None
    clap_error: str | None = Field(default=None, description="CLAP scoring error if any")

    # LLM scorer metrics (if computed)
    llm_vibe_match: float | None = None
    llm_audio_quality: float | None = None
    llm_creativity: float | None = None
    llm_justification: str | None = None
    llm_score: float | None = None  # Weighted overall score
    llm_error: str | None = Field(default=None, description="LLM scoring error if any")

    # Error tags for filtering/analysis
    error_tags: list[ErrorTag] = Field(
        default_factory=list,
        description="Tags indicating what errors occurred during evaluation",
    )

    # Timing
    inference_time_ms: float | None = None
    synth_time_ms: float | None = None


class EvalSetMetrics(BaseModel):
    """Aggregate metrics for an entire evaluation set."""

    model_config = ConfigDict(extra="forbid")

    eval_set_name: str
    source_label: str
    n_samples: int

    # Validity
    json_valid_rate: float = 0.0
    schema_valid_rate: float = 0.0
    field_accuracy: dict[str, float] = Field(default_factory=dict)

    # CLAP (if computed)
    clap_score_mean: float | None = None
    clap_score_std: float | None = None
    clap_score_median: float | None = None

    # LLM scorer (if computed)
    llm_score_mean: float | None = None
    llm_score_std: float | None = None
    llm_vibe_match_mean: float | None = None
    llm_audio_quality_mean: float | None = None
    llm_creativity_mean: float | None = None

    # Error counts
    n_config_errors: int = 0
    n_clap_errors: int = 0
    n_llm_errors: int = 0
    error_tag_counts: dict[str, int] = Field(default_factory=dict)

    # Diversity
    config_entropy: float | None = None
    field_distributions: dict[str, dict[str, int]] = Field(default_factory=dict)

    # Latency
    inference_time_mean_ms: float | None = None
    inference_time_p95_ms: float | None = None

    # Controllability (for controllability eval sets)
    attribute_accuracy: dict[str, float] = Field(default_factory=dict)


def compute_field_accuracy(
    results: list[EvalResult],
    prompts: list[EvalPrompt],
) -> dict[str, float]:
    """Compute per-field accuracy from eval results."""
    prompt_map = {p.id: p for p in prompts}
    field_correct: dict[str, int] = {}
    field_total: dict[str, int] = {}

    for result in results:
        prompt = prompt_map.get(result.prompt_id)
        if not prompt or not prompt.expected_fields:
            continue

        for field, expected in prompt.expected_fields.items():
            field_total[field] = field_total.get(field, 0) + 1
            if result.field_matches.get(field, False):
                field_correct[field] = field_correct.get(field, 0) + 1

    return {
        field: field_correct.get(field, 0) / total if total > 0 else 0.0
        for field, total in field_total.items()
    }


def compute_field_distributions(results: list[EvalResult]) -> dict[str, dict[str, int]]:
    """Compute distribution of values for each config field."""
    distributions: dict[str, dict[str, int]] = {}

    for result in results:
        if not result.config:
            continue

        for field, value in result.config.items():
            if field not in distributions:
                distributions[field] = {}
            str_value = str(value)
            distributions[field][str_value] = distributions[field].get(str_value, 0) + 1

    return distributions
