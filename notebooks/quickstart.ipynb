{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LatentScore Quickstart\n\n**Generate ambient music from text. No GPU required.**\n\nThis notebook walks through the core LatentScore API:\n1. Render from a vibe string\n2. Full control with `MusicConfig`\n3. Tweak with `MusicConfigUpdate` and `Step`\n4. Stream multiple vibes with crossfade\n5. Live streaming with a generator\n6. Bring Your Own LLM (`external:` models)\n\n[![Try in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/prabal-rje/latentscore/blob/main/notebooks/quickstart.ipynb)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q latentscore"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import latentscore as ls\n",
    "from IPython.display import Audio as IPAudio\n",
    "\n",
    "\n",
    "def listen(audio: ls.Audio) -> IPAudio:\n",
    "    \"\"\"Helper to play audio inline in the notebook.\"\"\"\n",
    "    return IPAudio(audio.samples, rate=audio.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Render from a vibe string\n",
    "\n",
    "The simplest API: describe what you want, get audio back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = ls.render(\"warm sunset over water\", duration=10.0)\n",
    "listen(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different vibes\n",
    "listen(ls.render(\"jazz cafe at midnight\", duration=10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listen(ls.render(\"thunderstorm on a tin roof\", duration=10.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## CLAP Audio-Embedding Model (`fast_heavy`)\n\nThe `fast_heavy` model uses LAION-CLAP to match your vibe text against pre-computed audio embeddings of each config's rendered audio. This matches text against what configs actually *sound* like, rather than comparing text-to-text.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# CLAP audio-embedding model (matches text against rendered audio)\nlisten(ls.render(\"warm sunset over water\", model=\"fast_heavy\", duration=10.0))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Full control with MusicConfig\n",
    "\n",
    "For precise control, build a `MusicConfig` directly with human-readable labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ls.MusicConfig(\n",
    "    tempo=\"slow\",\n",
    "    brightness=\"dark\",\n",
    "    space=\"vast\",\n",
    "    density=3,\n",
    "    bass=\"drone\",\n",
    "    pad=\"ambient_drift\",\n",
    "    melody=\"contemplative\",\n",
    "    rhythm=\"minimal\",\n",
    "    texture=\"shimmer\",\n",
    "    echo=\"heavy\",\n",
    "    root=\"d\",\n",
    "    mode=\"minor\",\n",
    ")\n",
    "\n",
    "audio = ls.render(config, duration=10.0)\n",
    "listen(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tweak with MusicConfigUpdate\n",
    "\n",
    "Start from a vibe and nudge specific parameters.\n",
    "\n",
    "### Absolute update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = ls.render(\n",
    "    \"morning coffee shop\",\n",
    "    duration=10.0,\n",
    "    update=ls.MusicConfigUpdate(\n",
    "        brightness=\"very_bright\",\n",
    "        rhythm=\"electronic\",\n",
    "    ),\n",
    ")\n",
    "listen(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative step update\n",
    "\n",
    "`Step(+1)` moves one level up the scale, `Step(-1)` moves one down. Saturates at boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latentscore.config import Step\n",
    "\n",
    "audio = ls.render(\n",
    "    \"morning coffee shop\",\n",
    "    duration=10.0,\n",
    "    update=ls.MusicConfigUpdate(\n",
    "        brightness=Step(+2),  # two levels brighter\n",
    "        space=Step(-1),       # one level less spacious\n",
    "    ),\n",
    ")\n",
    "listen(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stream multiple vibes with crossfade\n",
    "\n",
    "Chain vibes together with smooth transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = ls.stream(\n",
    "    \"morning coffee\",\n",
    "    \"afternoon focus\",\n",
    "    \"evening wind-down\",\n",
    "    duration=15,       # 15 seconds per vibe\n",
    "    transition=3.0,    # 3-second crossfade\n",
    ")\n",
    "\n",
    "collected = stream.collect()\n",
    "listen(collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Live streaming with a generator\n\n`ls.live()` accepts a sync or async generator that yields vibes, `MusicConfigUpdate`s, or `Track`s on the fly. Use `.play()` locally or `.collect()` in a notebook.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import asyncio\nfrom collections.abc import AsyncIterator\n\n\nasync def my_set() -> AsyncIterator[str | ls.MusicConfigUpdate]:\n    \"\"\"Yield vibes and config tweaks — the live engine crossfades between them.\"\"\"\n\n    yield \"warm jazz cafe at midnight\"\n    await asyncio.sleep(8)\n\n    # Absolute override: switch to bright electronic\n    yield ls.MusicConfigUpdate(tempo=\"fast\", brightness=\"very_bright\", rhythm=\"electronic\")\n    await asyncio.sleep(8)\n\n    # Relative nudge: dial brightness back down, add more echo\n    yield ls.MusicConfigUpdate(brightness=Step(-2), echo=Step(+1))\n\n\nsession = ls.live(my_set(), transition_seconds=2.0)\n\n# .play() streams to speakers (local only); .collect() buffers for notebook playback\naudio = session.collect(seconds=30)\nlisten(audio)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Bring Your Own LLM\n\nUse any LLM via [LiteLLM](https://docs.litellm.ai/docs/providers) to interpret vibes instead of embedding lookup. LiteLLM is included with latentscore — no extra install needed.\n\n**Set your API key first** (uncomment the provider you want):"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Uncomment ONE of these and paste your key:\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"your-key-here\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-key-here\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-key-here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini (free tier available)\n",
    "audio = ls.render(\n",
    "    \"cyberpunk rain on neon streets\",\n",
    "    model=\"external:gemini/gemini-3-flash-preview\",\n",
    "    duration=10.0,\n",
    ")\n",
    "listen(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing LLM metadata\n",
    "\n",
    "External models return rich metadata: title, reasoning, config, and color palettes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if audio.metadata is not None:\n",
    "    print(f\"Title:    {audio.metadata.title}\")\n",
    "    print(f\"Thinking: {audio.metadata.thinking[:200]}...\")\n",
    "    print(f\"Config:   tempo={audio.metadata.config.tempo}, brightness={audio.metadata.config.brightness}\")\n",
    "    for i, palette in enumerate(audio.metadata.palettes):\n",
    "        print(f\"Palette {i+1}: {[c.hex for c in palette.colors]}\")\n",
    "else:\n",
    "    print(\"No metadata (fast model doesn't produce metadata)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to file\n",
    "\n",
    "Any `Audio` object can be saved as a WAV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = ls.render(\"lo-fi study beats\", duration=10.0)\n",
    "audio.save(\"lofi.wav\")\n",
    "print(f\"Saved {len(audio.samples)} samples to lofi.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}